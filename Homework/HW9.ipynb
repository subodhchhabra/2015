{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AM 207**: Homework 9 - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_ _ _ _ _\n",
    "\n",
    "Verena Kaynig-Fittkau & Pavlos Protopapas <br>\n",
    "Handed out: Wednesday, April 8, 2015<br>\n",
    "Due: 11.59 P.M. Wednesday, April 15, 2015<br>\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "+ Upload your answers in an ipython notebook to the dropbox.\n",
    "\n",
    "+ Your individual submissions should use the following filename: AM207_YOURNAME_HW9.ipynb\n",
    "\n",
    "+ Your code should be in code cells as part of your ipython notebook. Do not use a different language (or format) unless you get permission from the TFs. If you use any special libraries you must include them with your code (program should run as is). \n",
    "\n",
    "+ If you have multiple files (e.g. you've added code files and images) create a tarball for all files in a single file and name it: AM207_YOURNAME_HW9.tar or AM207_YOURNAME_HW9.zip\n",
    "\n",
    "+ Please remember that your solution should be a report! We would like some explanations of your ideas, and ways to approach the solution. Also please comment your code.\n",
    "\n",
    "** Important **\n",
    "\n",
    "As you are all busy working on your final projects, HW9 and HW10 are half problem sets. This means that each HW has only one problem to solve. There will be a dropbox for each HW, but they will be graded together. If you still have sufficient late days, you can use one late day for HW9 and one late day for HW10. \n",
    "_ _ _ _ _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import seaborn as sns\n",
    "#sns.set_style('white')\n",
    "#sns.set_context('paper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Sam let me be\n",
    "\n",
    "[Green Eggs and Ham](http://en.wikipedia.org/wiki/Green_Eggs_and_Ham) is a best-selling and critically acclaimed children's book by Dr. Seuss, first published on August 12, 1960. As of 2001, according to Publishers Weekly, it was the fourth best-selling English-language children's book of all time.\n",
    "\n",
    "The book is actually the result of a bet between Dr Seuss and his publisher Bennett Cerf. Cerf bet Seuss that he could not complete an entire book using only fifty different words. \n",
    "\n",
    "You can find the entire text of the book in the file eggs.txt. The file is conveniently formatted to not include any punctuation signs. \n",
    "\n",
    "### Part 1: \n",
    "\n",
    "Use the text of the children's book to build a simple statistical language model for the writing style of green eggs and ham. \n",
    "Your model should be a _simple Markov chain of order 1_. Use the model to generate some sentences that are longer than the standard sentences in the file. \n",
    "\n",
    "Note: Each line in the text file is assumed to contain exactly one sentence. \n",
    "\n",
    "* Filter the text to get all fifty unique words of the text\n",
    "* Generate a vector for the starting probabilities of the chain (the first word of the sentence). \n",
    "* Generate a matrix with the transition probabilities of all words. \n",
    "* Add an additional row and column to your matrix to stand for the end of the sentence. Choose whatever symbol you like, I went for a '.' . The last word of a sentence should transition to this symbol and it only transitions to itself. \n",
    "* Generate and print 10 different sequences of 20 states from your model. This should correspond to 10 sentences, which might end in multiple '.', or also not end yet within the 20 states. \n",
    "\n",
    "Hint: It is best to convert all words to lower case. \n",
    "\n",
    "### Part 2:\n",
    "\n",
    "Extend your model to a Hidden Markov Model, that can correct spelling mistakes. The true word is the hidden state $X_i$ and the observed misspelled word is the $Y_i$. Identify the correct spelling by using the Viterbi algorithm to find the most probable chain of states $X$ that caused the observations $Y$. \n",
    "\n",
    "You already have the starting and transition probabilities from part 1. For an HMM you also need emission probabilities. We model the emission probabilities according to a form of edit distance: \n",
    "\n",
    "$$p(Y_i | X_i) = \\begin{cases}\n",
    "                       Poisson(k, \\lambda),& \\text{if } length(Y_i) == length(X_i)\\\\\n",
    "                        0,              & \\text{otherwise}\n",
    "\\end{cases}.$$ \n",
    "\n",
    "Here $k = d(X_i,Y_i)$ is the number of characters that are misspelled, for example $d(Sam, Tom) = 2$. You can play around a bit with the value for $\\lambda$, if you don't want to play around a value of 0.01 should work fine.  \n",
    "\n",
    "Use the Viterbi algorithm to correct the following sentences, and print the corrected version. There is a version of the Viterbi algorithm in the lecture notes that you can use for your reference. Please adjust the code to deal with numerical underflow and add a lot of comments that show you understand what is going on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observations1 = ('nat','im','a','cor','.')\n",
    "observations2 = ('yoo','lat','ma','be','.')\n",
    "observations3 = ('i', 'do', 'nat', 'eet', 'tnem', 'san', 'i', 'do', 'nat', 'like', 'grean', 'egxs', 'ant', 'hom', '.')\n",
    "observations4 = ('xxx','will','see','.')\n",
    "observations5 = ('do','you','like','xxxxx','xxxx','and','xxx','.')\n",
    "observations6 = ('x', 'xx', 'xxx', 'xxx', 'xxxx', 'xxx', 'x', 'xx', 'xxx', 'xxxx', 'xxxxx', 'xxxx', 'xxx', 'xxx', '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional\n",
    "\n",
    "If you would like to extend your knowledge of HMM models with this example, feel free to build models on the level of single characters, and build models of higher order. You could also try different texts, for example Shakespeare's Sonnets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
