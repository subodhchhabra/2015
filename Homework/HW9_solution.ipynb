{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "name": "",
  "signature": "sha256:7a8626336ce8148ab972ea26dea6f2889246f49b44f21ec8ce3def9d89b30afe"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "**AM 207**: Homework 9 - Solution"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import random\n",
      "import math\n",
      "import time\n",
      "import seaborn as sns\n",
      "#sns.set_style('white')\n",
      "#sns.set_context('paper')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Question 1: Sam let me be\n",
      "\n",
      "[Green Eggs and Ham](http://en.wikipedia.org/wiki/Green_Eggs_and_Ham) is a best-selling and critically acclaimed children's book by Dr. Seuss, first published on August 12, 1960. As of 2001, according to Publishers Weekly, it was the fourth best-selling English-language children's book of all time.\n",
      "\n",
      "The book is actually the result of a bet between Dr Seuss and his publisher Bennett Cerf. Cerf bet Seuss that he could not complete an entire book using only fifty different words. \n",
      "\n",
      "You can find the entire text of the book in the file eggs.txt. The file is conveniently formatted to not include any punctuation signs. \n",
      "\n",
      "### Part 1: \n",
      "\n",
      "Use the text of the children's book to build a simple statistical language model for the writing style of green eggs and ham. \n",
      "Your model should be a _simple Markov chain of order 1_. Use the model to generate some sentences that are longer than the standard sentences in the file. \n",
      "\n",
      "Note: Each line in the text file is assumed to contain exactly one sentence. \n",
      "\n",
      "* Filter the text to get all fifty unique words of the text\n",
      "* Generate a vector for the starting probabilities of the chain (the first word of the sentence). \n",
      "* Generate a matrix with the transition probabilities of all words. \n",
      "* Add an additional row and column to your matrix to stand for the end of the sentence. Choose whatever symbol you like, I went for a '.' . The last word of a sentence should transition to this symbol and it only transitions to itself. \n",
      "* Generate and print 10 different sequences of 20 states from your model. This should correspond to 10 sentences, which might end in multiple '.', or also not end yet within the 20 states. \n",
      "\n",
      "Hint: It is best to convert all words to lower case. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First we just have a look at the text in the file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('eggs.txt') as f:\n",
      "    content = f.readlines()\n",
      "    \n",
      "content"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "['I am Sam\\r\\n',\n",
        " 'Sam I am\\r\\n',\n",
        " 'That Sam I am\\r\\n',\n",
        " 'That Sam I am\\r\\n',\n",
        " 'I do not like that Sam I am\\r\\n',\n",
        " 'Do you like green eggs and ham\\r\\n',\n",
        " 'I do not like them Sam I am\\r\\n',\n",
        " 'I do not like green eggs and ham\\r\\n',\n",
        " 'Would you like them here or there\\r\\n',\n",
        " 'I would not like them here or there\\r\\n',\n",
        " 'I would not like them anywhere\\r\\n',\n",
        " 'I do not like green eggs and ham\\r\\n',\n",
        " 'I do not like them Sam I am\\r\\n',\n",
        " 'Would you like them in a house\\r\\n',\n",
        " 'Would you like them with a mouse\\r\\n',\n",
        " 'I do not like them in a house\\r\\n',\n",
        " 'I do not like them with a mouse\\r\\n',\n",
        " 'I do not like them here or there\\r\\n',\n",
        " 'I do not like them anywhere\\r\\n',\n",
        " 'I do not like green eggs and ham\\r\\n',\n",
        " 'I do not like them  Sam I am\\r\\n',\n",
        " 'Would you eat them in a box\\r\\n',\n",
        " 'Would you eat them with a fox\\r\\n',\n",
        " 'Not in a box\\r\\n',\n",
        " 'Not with a fox\\r\\n',\n",
        " 'Not in a house\\r\\n',\n",
        " 'Not with a mouse\\r\\n',\n",
        " 'I would not eat them here or there\\r\\n',\n",
        " 'I would not eat them anywhere\\r\\n',\n",
        " 'I would not eat green eggs and ham\\r\\n',\n",
        " 'I do not like them  Sam I am\\r\\n',\n",
        " 'Would you \\r\\n',\n",
        " 'Could you\\r\\n',\n",
        " 'in a car\\r\\n',\n",
        " 'Eat them\\r\\n',\n",
        " 'Eat them \\r\\n',\n",
        " 'Here they are\\r\\n',\n",
        " 'I would not could not in a car\\r\\n',\n",
        " 'You may like them\\r\\n',\n",
        " 'You will see\\r\\n',\n",
        " 'You may like them in a tree\\r\\n',\n",
        " 'I would not could not in a tree\\r\\n',\n",
        " 'Not in a car\\r\\n',\n",
        " 'You let me be\\r\\n',\n",
        " 'I do not like them in a box\\r\\n',\n",
        " 'I do not like them with a fox\\r\\n',\n",
        " 'I do not like them in a house\\r\\n',\n",
        " 'I do not like them with a mouse\\r\\n',\n",
        " 'I do not like them here or there\\r\\n',\n",
        " 'I do not like them anywhere\\r\\n',\n",
        " 'I do not like green eggs and ham\\r\\n',\n",
        " 'I do not like them Sam I am\\r\\n',\n",
        " 'A train\\r\\n',\n",
        " 'A train \\r\\n',\n",
        " 'A train\\r\\n',\n",
        " 'A train \\r\\n',\n",
        " 'Could you  would you on a train\\r\\n',\n",
        " 'Not on a train\\r\\n',\n",
        " 'Not in a tree \\r\\n',\n",
        " 'Not in a car\\r\\n',\n",
        " 'Sam Let me be \\r\\n',\n",
        " 'I would not could not in a box\\r\\n',\n",
        " 'I could not would not with a fox\\r\\n',\n",
        " 'I will not eat them with a mouse\\r\\n',\n",
        " 'I will not eat them in a house\\r\\n',\n",
        " 'I will not eat them here or there\\r\\n',\n",
        " 'I will not eat them anywhere\\r\\n',\n",
        " 'I do not like them Sam I am\\r\\n',\n",
        " 'Say \\r\\n',\n",
        " 'In the dark\\r\\n',\n",
        " 'Here in the dark\\r\\n',\n",
        " 'Would you could you in the dark\\r\\n',\n",
        " 'I would not could not in the dark\\r\\n',\n",
        " 'Would you could you in the rain\\r\\n',\n",
        " 'I would not could not in the rain\\r\\n',\n",
        " 'Not in the dark\\r\\n',\n",
        " 'Not on a train \\r\\n',\n",
        " 'Not in a car\\r\\n',\n",
        " 'Not in a tree\\r\\n',\n",
        " 'I do not like them Sam you see\\r\\n',\n",
        " 'Not in a house\\r\\n',\n",
        " 'Not in a box\\r\\n',\n",
        " 'Not with a mouse\\r\\n',\n",
        " 'Not with a fox\\r\\n',\n",
        " 'I will not eat them here or there\\r\\n',\n",
        " 'I do not like them anywhere\\r\\n',\n",
        " 'You do not like green eggs and ham\\r\\n',\n",
        " 'I do not like them Sam I am\\r\\n',\n",
        " 'Could you would you with a goat\\r\\n',\n",
        " 'I would not could not with a goat \\r\\n',\n",
        " 'Would you could you on a boat\\r\\n',\n",
        " 'I could not would not on a boat\\r\\n',\n",
        " 'I will not will not with a goat\\r\\n',\n",
        " 'I will not eat them in the rain\\r\\n',\n",
        " 'I will not eat them on a train\\r\\n',\n",
        " 'Not in the dark\\r\\n',\n",
        " 'Not in a tree\\r\\n',\n",
        " 'Not in a car\\r\\n',\n",
        " 'You let me be \\r\\n',\n",
        " 'I do not like them in a box\\r\\n',\n",
        " 'I do not like them with a fox\\r\\n',\n",
        " 'I will not eat them in a house\\r\\n',\n",
        " 'I do not like them with a mouse\\r\\n',\n",
        " 'I do not like them here or there\\r\\n',\n",
        " 'I do not like them ANYWHERE\\r\\n',\n",
        " 'I do not like green eggs and ham \\r\\n',\n",
        " 'I do not like them Sam I am\\r\\n',\n",
        " 'You do not like them\\r\\n',\n",
        " 'SO you say\\r\\n',\n",
        " 'Try them\\r\\n',\n",
        " 'Try them\\r\\n',\n",
        " 'ANd you may\\r\\n',\n",
        " 'Try them and you may I say\\r\\n',\n",
        " 'Sam\\r\\n',\n",
        " 'If you will let me be I will try them\\r\\n',\n",
        " 'You will see\\r\\n',\n",
        " 'Say\\r\\n',\n",
        " 'I like green eggs and ham \\r\\n',\n",
        " 'I do\\r\\n',\n",
        " 'I like them Sam I am \\r\\n',\n",
        " 'And I would eat them in a boat \\r\\n',\n",
        " 'And I would eat them with a goat \\r\\n',\n",
        " 'And I will eat them in the rain\\r\\n',\n",
        " 'And in the dark\\r\\n',\n",
        " 'And on a train\\r\\n',\n",
        " 'And in a car\\r\\n',\n",
        " 'And in a tree\\r\\n',\n",
        " 'They are so good so good you see \\r\\n',\n",
        " 'So I will eat them in a box\\r\\n',\n",
        " 'And I will eat them with a fox\\r\\n',\n",
        " 'And I will eat them in a house\\r\\n',\n",
        " 'And I will eat them with a mouse\\r\\n',\n",
        " 'And I will eat them here and there\\r\\n',\n",
        " 'Say\\r\\n',\n",
        " 'I will eat them anywhere\\r\\n',\n",
        " 'I do so like green eggs and ham \\r\\n',\n",
        " 'Thank you\\r\\n',\n",
        " 'Thank you Sam I am']"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As promised in the description, each line contains a sentence. The language is very simple and we can observe some dominant sentence structures, like 'I do not like them ' followed by some descirption like 'in a house' or 'with a mouse'. \n",
      "\n",
      "Now we filter all unique words from the text. Here we also just count how many times each word occured in the text. This is not required for our model though. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# unique word counts\n",
      "count = {}\n",
      "# go line by line through the file and loop over all words of each line\n",
      "for w in open('Seuss.txt').read().split():\n",
      "    #convert to lower case\n",
      "    w = w.lower()\n",
      "    #if we encountered the word before increase the count\n",
      "    if w in count:\n",
      "        count[w] += 1\n",
      "    #otherwise add the word to the dictionary\n",
      "    else:\n",
      "        count[w] = 1\n",
      "        \n",
      "#number of words should be 50\n",
      "print \"Number of words: \", len(count)\n",
      "print\n",
      "print count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of words:  50\n",
        "\n",
        "{'and': 24, 'sam': 18, 'be': 4, 'house': 8, 'am': 15, 'box': 7, 'see': 4, 'are': 2, 'in': 40, 'mouse': 8, 'boat': 3, 'if': 1, 'try': 4, 'ham': 10, 'would': 26, 'there': 9, 'fox': 7, 'so': 5, 'you': 34, 'goat': 4, 'do': 36, 'them': 61, 'good': 2, 'that': 3, 'may': 4, 'eggs': 10, 'here': 11, 'dark': 7, 'me': 4, 'train': 9, 'let': 4, 'rain': 4, 'they': 2, 'not': 83, 'with': 19, 'eat': 24, 'thank': 2, 'a': 59, 'on': 7, 'like': 44, 'i': 83, 'car': 7, 'could': 14, 'tree': 6, 'say': 5, 'will': 21, 'anywhere': 8, 'green': 10, 'the': 11, 'or': 8}\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For later reference we also create a lookup dictionary that contains a unique id for each word."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words = count.keys()\n",
      "words.sort()\n",
      "\n",
      "indices = {}\n",
      "i = 0\n",
      "for w in words:\n",
      "    indices[w] = i\n",
      "    i+=1\n",
      "\n",
      "indices['.'] = i\n",
      "print i\n",
      "print indices\n",
      "print indices['.']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "50\n",
        "{'and': 2, 'sam': 33, 'be': 5, 'house': 20, 'am': 1, 'box': 7, 'say': 34, 'are': 4, 'in': 23, 'mouse': 28, 'boat': 6, 'if': 22, 'will': 46, 'ham': 18, 'would': 48, 'there': 41, 'fox': 14, '.': 50, 'so': 36, 'you': 49, 'goat': 15, 'do': 11, 'them': 40, 'good': 16, 'that': 38, 'may': 26, 'eggs': 13, 'here': 19, 'dark': 10, 'me': 27, 'train': 43, 'let': 24, 'rain': 32, 'they': 42, 'not': 29, 'with': 47, 'eat': 12, 'thank': 37, 'a': 0, 'on': 30, 'like': 25, 'i': 21, 'car': 8, 'could': 9, 'tree': 44, 'see': 35, 'try': 45, 'anywhere': 3, 'green': 17, 'the': 39, 'or': 31}\n",
        "50\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We build the vector of starting probabilities. For completeness we create it with 51 entries, adding one entry for the sentence stop symbol '.'."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start_prob = np.zeros(51)\n",
      "with open('eggs.txt') as f:\n",
      "    for line in f:\n",
      "        words = line.split()\n",
      "        word_current = words[0].lower()\n",
      "        ind_current = indices[word_current]\n",
      "        start_prob[ind_current] =  start_prob[ind_current]+1\n",
      "\n",
      "# normalize \n",
      "start_prob = start_prob / np.sum(start_prob)\n",
      "print start_prob"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.02898551  0.          0.08695652  0.          0.          0.          0.\n",
        "  0.          0.          0.02173913  0.          0.00724638  0.01449275\n",
        "  0.          0.          0.          0.          0.          0.\n",
        "  0.01449275  0.          0.42753623  0.00724638  0.01449275  0.          0.\n",
        "  0.          0.          0.          0.13768116  0.          0.          0.\n",
        "  0.02173913  0.02173913  0.          0.01449275  0.01449275  0.01449275\n",
        "  0.          0.          0.          0.00724638  0.          0.\n",
        "  0.02173913  0.          0.          0.06521739  0.05797101  0.        ]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that only a subset of the words in the book ever appear at the beginning of a sentence. \n",
      "\n",
      "Now we need the transition probabilities. It is important to include a transition from the last word of a sentence to the '.' symbol. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get transition matrix\n",
      "\n",
      "transitions = np.zeros((51,51))\n",
      "with open('eggs.txt') as f:\n",
      "    for line in f:\n",
      "        words = line.split()\n",
      "        for i in xrange(len(words)):\n",
      "            # if we are not at the end of the sentence\n",
      "            # we add a transition to the next word\n",
      "            if i < len(words)-1:\n",
      "                word_current = words[i].lower()\n",
      "                word_next = words[i+1].lower()\n",
      "            \n",
      "            #last word of sentence transitions to '.'  \n",
      "            else: \n",
      "                word_current = words[i].lower()\n",
      "                word_next = '.'\n",
      "            \n",
      "            ind_current = indices[word_current]\n",
      "            ind_next = indices[word_next]\n",
      "            transitions[ind_current, ind_next] =  transitions[ind_current, ind_next]+1\n",
      "\n",
      "# each word now either has a transition to '.' or another word, so no division by 0\n",
      "# the '.' itself has no transition, so it stays in its current state. \n",
      "\n",
      "transitions[-1,-1] = 1.0\n",
      "row_sums = np.sum(transitions, axis=1)\n",
      "\n",
      "transitions = transitions / row_sums[:, np.newaxis]\n",
      "\n",
      "# check normalization\n",
      "print np.sum(transitions, axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
        "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
        "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we generate some sentences from our model. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# we discussed this in the context of genetic algorithms\n",
      "def fortune_wheel(choices): \n",
      "    s = np.cumsum(choices) / np.sum(choices)\n",
      "    u = np.random.uniform()\n",
      "    return np.sum(s<=u)\n",
      "\n",
      "# generate a sentence from the model\n",
      "def generate_sentence(length, transitions, start_prob):\n",
      "    #sample first word from start prob\n",
      "    s_prev = fortune_wheel(start_prob)\n",
      "    sentence = []\n",
      "    sentence.append(s_prev)\n",
      "    \n",
      "    for i in xrange(length-1):\n",
      "        t = transitions[s_prev,:]\n",
      "        s = fortune_wheel(t)\n",
      "        sentence.append(s)\n",
      "        s_prev = s\n",
      "    \n",
      "    return sentence\n",
      "    \n",
      "# converts list of indices to human readable words\n",
      "def decode_x(x, indices):\n",
      "    keys = indices.keys()\n",
      "    keys.sort()\n",
      "    \n",
      "    #rearranging for '.' to be at the end of the list\n",
      "    keys.pop(0)\n",
      "    keys.append('.')\n",
      "    \n",
      "    x = np.array(x)\n",
      "    \n",
      "    for i in xrange(x.shape[0]):\n",
      "        sentence = []\n",
      "        for j in xrange(x.shape[1]):\n",
      "            sentence.append(keys[x[i,j]])\n",
      "        print sentence"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "length = 20\n",
      "\n",
      "for i in xrange(10):\n",
      "    x = generate_sentence(length, transitions, start_prob)\n",
      "    decode_x(np.reshape(x,(1,length)), indices)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'i', 'do', 'not', 'eat', 'them', 'and', 'there', '.', '.', '.', '.', '.', '.']\n",
        "['i', 'would', 'you', 'like', 'them', 'here', 'and', 'in', 'a', 'tree', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
        "['could', 'not', 'in', 'the', 'rain', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
        "['i', 'am', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
        "['i', 'do', 'not', 'like', 'them', 'in', 'a', 'tree', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
        "['i', 'am', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
        "['i', 'am', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
        "['not', 'in', 'a', 'train', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
        "['i', 'will', 'see', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
        "['i', 'do', 'not', 'eat', 'them', 'here', 'or', 'there', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that we generate a sequence of 20 states from the model. As the '.' transitions to itself, the model will keep generating '.' once we reached that state. It is not a valid approach to generate a number of words and then arbitrarely add a '.' into the state sequence, as some words never occured at the end of a sentence and thus the transition probability from such a word to '.' is zero. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Part 2:\n",
      "\n",
      "Extend your model to a Hidden Markov Model, that can correct spelling mistakes. The true word is the hidden state $X_i$ and the observed misspelled word is the $Y_i$. Identify the correct spelling by using the Viterbi algorithm to find the most probable chain of states $X$ that caused the observations $Y$. \n",
      "\n",
      "You already have the starting and transition probabilities from part 1. For an HMM you also need emission probabilities. We model the emission probabilities according to a form of edit distance: \n",
      "\n",
      "$$p(Y_i | X_i) = \\begin{cases}\n",
      "                       Poisson(k, \\lambda),& \\text{if } length(Y_i) == length(X_i)\\\\\n",
      "                        0,              & \\text{otherwise}\n",
      "\\end{cases}.$$ \n",
      "\n",
      "Here $k = d(X_i,Y_i)$ is the number of characters that are misspelled, for example $d(Sam, Tom) = 2$. You can play around a bit with the value for $\\lambda$, if you don't want to play around a value of 0.01 should work fine.  \n",
      "\n",
      "Use the Viterbi algorithm to correct the following sentences, and print the corrected version. There is a version of the Viterbi algorithm in the lecture notes that you can use for your reference. Please adjust the code to deal with numerical underflow and add a lot of comments that show you understand what is going on. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "observations1 = ('nat','im','a','cor','.')\n",
      "observations2 = ('yoo','lat','ma','be','.')\n",
      "observations3 = ('i', 'do', 'nat', 'eet', 'tnem', 'san', 'i', 'do', 'nat', 'like', 'grean', 'egxs', 'ant', 'hom', '.')\n",
      "observations4 = ('xxx','will','see','.')\n",
      "observations5 = ('do','you','like','xxxxx','xxxx','and','xxx','.')\n",
      "observations6 = ('x', 'xx', 'xxx', 'xxx', 'xxxx', 'xxx', 'x', 'xx', 'xxx', 'xxxx', 'xxxxx', 'xxxx', 'xxx', 'xxx', '.')\n",
      "\n",
      "observations = [observations1, observations2, observations3, observations4, observations5, observations6]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# the code is from the lecture, possibilities to add numerical stability are \n",
      "# to work in log space, or to re-normalize accross the possible states. \n",
      "def viterbi_num(obs, states, start_p, trans_p, emission_probability, l):\n",
      "    \"\"\"The numerically stable Viterbi algorithm\n",
      "    obs - the observations\n",
      "    states - \n",
      "    start_p - start probability vector, the probability to start at each state\n",
      "    trans_p - transition probability matrix, the probability to move from one state to another\n",
      "    emission_probability - a pmf function to compute the distance between two states\n",
      "    l - lambda value for calculating the emission probability\n",
      "    \"\"\"\n",
      "    # \n",
      "    V = [{}]\n",
      "    path = {}\n",
      " \n",
      "    # Initialize base cases (t == 0)\n",
      "    for y in states:\n",
      "        # compute the value for all possible starting states\n",
      "        V[0][y] = start_p[y] * emission_probability(y,obs[0],l)\n",
      "        # remember all the different paths (here its only one state so far)\n",
      "        path[y] = [y]\n",
      " \n",
      "    # normalize for numerical stability\n",
      "    s = np.sum(V[0].values())\n",
      "    for t in V[0]:\n",
      "        V[0][t] = V[0][t] / s\n",
      "\n",
      "    # Run Viterbi for t > 0\n",
      "    for t in range(1, len(obs)):\n",
      "        V.append({})\n",
      "        newpath = {}\n",
      " \n",
      "        for y in states:\n",
      "            # compute the values coming from all possible previous states, only keep the maximum\n",
      "            (prob, state) = max((V[t-1][y0] * trans_p[y0][y] * emission_probability(y,obs[t],l), y0) for y0 in states)\n",
      "            # save the maximum value for each state\n",
      "            V[t][y] = prob\n",
      "            # remember the path we came from to get this maximum value\n",
      "            newpath[y] = path[state] + [y]\n",
      "            \n",
      "        # normalize for stability\n",
      "        s = np.sum(V[t].values())\n",
      "        for tt in V[t]:\n",
      "            V[t][tt] = V[t][tt] / s\n",
      " \n",
      "        # Don't need to remember the old paths\n",
      "        path = newpath\n",
      "     \n",
      "    (prob, state) = max((V[t][y], y) for y in states)\n",
      "    return path[state]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# the emission probability depends on the edit distance\n",
      "# it is zero in case the words don't have the same length\n",
      "def emission_probability(s1,s2, l):\n",
      "    if len(s1) != len(s2):\n",
      "        return 0.0\n",
      "    \n",
      "    count = 0\n",
      "    for c1,c2 in zip(s1,s2):\n",
      "        count += c1!=c2\n",
      "        \n",
      "    p = sp.stats.poisson.pmf(count, l)\n",
      "    return p\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To use the lecture code we need to redefine our variables as dictionaries. We can either do this or alter the Viterbi code to work with matrices, whichever you prefer. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "states = tuple(indices.keys())\n",
      "\n",
      "keys = indices.keys()\n",
      "keys.sort() \n",
      "keys.pop(0)\n",
      "keys.append('.')\n",
      "\n",
      "start_probability = {}\n",
      "\n",
      "for s,k in zip(start_prob, keys):\n",
      "    start_probability[k]= s\n",
      "    \n",
      "transition_probability = {}\n",
      "\n",
      "for i in xrange(51):\n",
      "    tp = {}\n",
      "    for j in xrange(51):\n",
      "        tp[keys[j]] = transitions[i,j]\n",
      "    transition_probability[keys[i]] = tp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for o in observations:\n",
      "    print o\n",
      "    print viterbi_num(o, states, start_probability, transition_probability, emission_probability, 0.01)\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('nat', 'im', 'a', 'cor', '.')\n",
        "['not', 'in', 'a', 'car', '.']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "('yoo', 'lat', 'ma', 'be', '.')\n",
        "['you', 'let', 'me', 'be', '.']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "('i', 'do', 'nat', 'eet', 'tnem', 'san', 'i', 'do', 'nat', 'like', 'grean', 'egxs', 'ant', 'hom', '.')\n",
        "['i', 'do', 'not', 'eat', 'them', 'sam', 'i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "('xxx', 'will', 'see', '.')\n",
        "['you', 'will', 'see', '.']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "('do', 'you', 'like', 'xxxxx', 'xxxx', 'and', 'xxx', '.')\n",
        "['do', 'you', 'like', 'green', 'eggs', 'and', 'ham', '.']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "('x', 'xx', 'xxx', 'xxx', 'xxxx', 'xxx', 'x', 'xx', 'xxx', 'xxxx', 'xxxxx', 'xxxx', 'xxx', 'xxx', '.')\n",
        "['i', 'do', 'not', 'eat', 'them', 'sam', 'i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our model seems to be pretty powerful in correcting spelling mistakes. The reason is that the language of the book is very simplified and we restricted our edit distance to not allow for words of different length. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    }
   ],
   "metadata": {}
  }
 ]
}